#! /bin/bash

echo "Preparing:"
set -x                          # Output commands
set -e                          # Abort on errors

cd @RUNDIR@-active

echo "Checking:"
pwd
hostname
date

echo "Environment:"

module purge
module load compiler/intel/17.0
module load mpi/openmpi/1.10-gnu-6.1
module load numlib/mkl/2017
module load numlib/gsl/2.1
#module load numlib/fftw/3.3-openmpi-1.10.3-gnu-4.9
#module load lib/hdf5/1.8.16-gnu-4.9
module load devel/automake/1.15
#module load compiler/gnu/6.1

export CACTUS_NUM_PROCS=@NUM_PROCS@
export CACTUS_NUM_THREADS=@NUM_THREADS@
#export GMON_OUT_PREFIX=gmon.out
export OMP_NUM_THREADS=@NUM_THREADS@
export OPENMPI_DIR=$MPI_HOME
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH
#LD_LIBRARY_PATH =  /opt/bwhpc/common/compiler/gnu/6.1/lib64:$LD_LIBRARY_PATH

env | sort > SIMFACTORY/ENVIRONMENT

env | sort > SIMFACTORY/ENVIRONMENT
echo "Job setup:"
echo "   Allocated:"
echo "      Nodes:                      @NODES@"
echo "      Cores per node:             @PPN@"
echo "   Running:"
echo "      MPI processes:              @NUM_PROCS@"
echo "      OpenMP threads per process: @NUM_THREADS@"
echo "      MPI processes per node:     @(1.0*@NUM_PROCS@/@NODES@)@"
echo "      OpenMP threads per core:    @(1.0*(@NUM_PROCS@*@NUM_THREADS@)/(@NODES@*@PPN@))@"
echo "      OpenMP threads per node:    @PPN_USED@"

# Print schedule
@EXECUTABLE@ -S @PARFILE@ &> schedule
cat ${PBS_NODEFILE} | sort -u > NODES
#echo ${SLURM_NODELIST} > NODES

echo "Starting:"
export CACTUS_STARTTIME=$(date +%s)

MPIRUN_OPTIONS="--bind-to core --map-by socket:PE=@NUM_THREADS@ -report-bindings"
MPI_PER_NODE=@(1.0*@NUM_PROCS@/@NODES@)@


#time $OPENMPI_DIR/bin/mpirun -np @NUM_PROCS@ -machinefile ${MPI_NODEFILE} -x OMP_NUM_THREADS=@NUM_THREADS@ @EXECUTABLE@ -L 3 @PARFILE@
time $OPENMPI_DIR/bin/mpirun -n @NUM_PROCS@ ${MPIRUN_OPTIONS} @EXECUTABLE@ -redirect=oe -L 3 @PARFILE@

echo "Stopping:"
date

echo "Done."
